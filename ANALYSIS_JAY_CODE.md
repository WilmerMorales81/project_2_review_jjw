# Jay çš„ä»£ç åˆ†æï¼šS3 æœåŠ¡æ¨¡å—

## ğŸ“‹ æ•´ä½“è®¾è®¡æ€è·¯

Jay åˆ›å»ºäº†ä¸€ä¸ª **æ¨¡å—åŒ–çš„ S3 æ•°æ®ç®¡ç†ç³»ç»Ÿ**ï¼Œæ ¸å¿ƒç†å¿µæ˜¯ï¼š
1. **è§£è€¦æ•°æ®å­˜å‚¨** - å°†æ•°æ®ä»æœ¬åœ°è¿ç§»åˆ°äº‘ç«¯
2. **æ”¯æŒå¤§æ•°æ®** - ä½¿ç”¨æµå¼å¤„ç†å’Œä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
3. **çµæ´»æ€§** - æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆCSV, Parquetï¼‰å’Œå¤„ç†æ¨¡å¼ï¼ˆeager/lazyï¼‰

---

## ğŸ”§ æ ¸å¿ƒå‡½æ•°è§£æ

### 1. `get_s3_session()` - S3 ä¼šè¯ç®¡ç†
```python
def get_s3_session(profile_name: str = None):
    if profile_name:
        session = boto3.Session(profile_name=profile_name)
    else:
        session = boto3.Session()
    return session.client('s3')
```

**è®¾è®¡æ€è·¯ï¼š**
- âœ… **å•ä¸€èŒè´£** - åªè´Ÿè´£åˆ›å»º S3 å®¢æˆ·ç«¯
- âœ… **çµæ´»è®¤è¯** - æ”¯æŒå¤šä¸ª AWS profileï¼ˆå›¢é˜Ÿåä½œï¼‰
- âœ… **å¯å¤ç”¨** - è¢«å…¶ä»–å‡½æ•°è°ƒç”¨ï¼Œé¿å…é‡å¤ä»£ç 

**ä½¿ç”¨åœºæ™¯ï¼š**
```python
# ä½¿ç”¨é»˜è®¤ profile
s3 = get_s3_session()

# ä½¿ç”¨ç‰¹å®š profileï¼ˆå¤šè´¦æˆ·ç®¡ç†ï¼‰
s3_prod = get_s3_session('production')
s3_dev = get_s3_session('development')
```

---

### 2. `upload_polars_to_s3()` - å¿«é€Ÿä¸Šä¼ ï¼ˆCSVï¼‰
```python
def upload_polars_to_s3(df: pl.DataFrame, bucket: str, key: str, profile_name: str = None):
    s3 = get_s3_session(profile_name)

    # å†…å­˜ä¸­è½¬æ¢ä¸º CSV
    csv_buffer = StringIO()
    df.write_csv(csv_buffer)
    csv_buffer.seek(0)

    # ç›´æ¥ä¸Šä¼ 
    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())
    logger.info(f"Uploaded {df.height} rows to s3://{bucket}/{key}")
```

**è®¾è®¡æ€è·¯ï¼š**
- âœ… **å†…å­˜æ“ä½œ** - ä½¿ç”¨ StringIO åœ¨å†…å­˜ä¸­å¤„ç†ï¼Œé€Ÿåº¦å¿«
- âœ… **é€‚åˆå°æ•°æ®é›†** - ä¸é€‚åˆè¶…å¤§æ–‡ä»¶ï¼ˆä¼šå ç”¨å†…å­˜ï¼‰
- âœ… **æ—¥å¿—è®°å½•** - ä½¿ç”¨ logger è¿½è¸ªä¸Šä¼ çŠ¶æ€

**é€‚ç”¨åœºæ™¯ï¼š**
- å°åˆ°ä¸­ç­‰æ•°æ®é›†ï¼ˆ< 1GBï¼‰
- éœ€è¦å¿«é€Ÿä¸Šä¼ 
- CSV æ ¼å¼å…¼å®¹æ€§å¥½

**Jay çš„ç¼–ç ä¼˜ç‚¹ï¼š**
- `csv_buffer.seek(0)` - é‡ç½®æŒ‡é’ˆï¼Œç»†èŠ‚åˆ°ä½
- ä½¿ç”¨ `df.height` è€Œä¸æ˜¯ `len(df)` - Polars çš„æ ‡å‡†å†™æ³•

---

### 3. `write_parquet_to_s3()` - å¤§æ•°æ®ä¼˜åŒ–ä¸Šä¼ 

è¿™æ˜¯ **Jay çš„æ ¸å¿ƒåˆ›æ–°**ï¼

```python
def write_parquet_to_s3(
    df: Union[pl.DataFrame, pl.LazyFrame],
    bucket: str,
    key: str,
    profile: str,
):
    session = boto3.Session(profile_name=profile)
    s3 = session.client("s3")

    # ğŸ”‘ å…³é”®1ï¼šæµå¼å¤„ç†å¤§æ•°æ®
    if isinstance(df, pl.LazyFrame):
        df = df.collect(streaming=True)

    # ğŸ”‘ å…³é”®2ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…å†…å­˜é—®é¢˜
    with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as tmp:
        df.write_parquet(tmp.name)
        tmp_path = tmp.name

    # ğŸ”‘ å…³é”®3ï¼šè·¯å¾„è§„èŒƒåŒ–
    s3_key = f"{key.rstrip('/')}/data.parquet"

    # ğŸ”‘ å…³é”®4ï¼šä½¿ç”¨ upload_fileobjï¼ˆæ”¯æŒå¤§æ–‡ä»¶ï¼‰
    with open(tmp_path, "rb") as f:
        s3.upload_fileobj(f, bucket, s3_key)

    # ğŸ”‘ å…³é”®5ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(tmp_path)

    return f"s3://{bucket}/{s3_key}"
```

**è®¾è®¡æ€è·¯æ·±åº¦åˆ†æï¼š**

#### ğŸ’¡ ä¸ºä»€ä¹ˆç”¨ä¸´æ—¶æ–‡ä»¶ï¼Ÿ
```
æ²¡æœ‰ä¸´æ—¶æ–‡ä»¶çš„é—®é¢˜ï¼š
æ•°æ® â†’ å†…å­˜ â†’ S3
      â†‘ å¦‚æœæ•°æ® 10GBï¼Œå†…å­˜çˆ†ç‚¸ï¼

ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼š
æ•°æ® â†’ ä¸´æ—¶æ–‡ä»¶ â†’ S3
      â†‘ ç£ç›˜ç¼“å†²ï¼Œå†…å­˜å®‰å…¨
```

#### ğŸ’¡ ä¸ºä»€ä¹ˆç”¨ `upload_fileobj` è€Œä¸æ˜¯ `put_object`ï¼Ÿ
| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `put_object` | ç®€å•ç›´æ¥ | æ–‡ä»¶éœ€å®Œå…¨åŠ è½½åˆ°å†…å­˜ | < 100MB |
| `upload_fileobj` | è‡ªåŠ¨åˆ†ç‰‡ä¸Šä¼  | ç¨å¾®å¤æ‚ | > 100MB |

**Jay çš„é€‰æ‹©ï¼š** ä½¿ç”¨ `upload_fileobj` = **è€ƒè™‘åˆ°é¡¹ç›®æ•°æ®é‡å¤§**ï¼ˆ9.9GBï¼‰

#### ğŸ’¡ ä¸ºä»€ä¹ˆæ”¯æŒ LazyFrameï¼Ÿ
```python
# ä¼ ç»Ÿæ–¹å¼ï¼ˆå†…å­˜å‹åŠ›å¤§ï¼‰
df = pl.read_csv("huge_file.csv")  # å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
process(df)

# LazyFrame æ–¹å¼ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
df = pl.scan_csv("huge_file.csv")  # ä¸åŠ è½½ï¼Œåªè®°å½•æ“ä½œ
df = df.filter(...)  # å»¶è¿Ÿæ‰§è¡Œ
df = df.collect(streaming=True)  # æµå¼å¤„ç†
```

**Jay çš„æ™ºæ…§ï¼š** æ”¯æŒä¸¤ç§æ¨¡å¼ï¼Œç»™ä½¿ç”¨è€…é€‰æ‹©æƒ

---

### 4. `scan_parquet_from_s3()` - äº‘ç«¯æ‡’åŠ è½½

```python
def scan_parquet_from_s3(
    bucket: str,
    key: str,
    profile: str,
) -> pl.LazyFrame:
    session = boto3.Session(profile_name=profile)

    # ğŸ”‘ å…³é”®ï¼šå›ºå®šå‡­è¯ï¼ˆé˜²æ­¢è¿‡æœŸï¼‰
    creds = session.get_credentials().get_frozen_credentials()

    storage_options = {
        "aws_access_key_id": creds.access_key,
        "aws_secret_access_key": creds.secret_key,
        "aws_session_token": creds.token,
        "region": session.region_name,
    }

    s3_path = f"s3://{bucket}/{key.rstrip('/')}/*.parquet"

    return pl.scan_parquet(
        s3_path,
        storage_options=storage_options
    )
```

**è®¾è®¡æ€è·¯ï¼š**
- âœ… **å»¶è¿ŸåŠ è½½** - è¿”å› LazyFrameï¼Œä¸ç«‹å³è¯»å–æ•°æ®
- âœ… **å‡­è¯ç®¡ç†** - ä½¿ç”¨ `get_frozen_credentials()` é˜²æ­¢ token è¿‡æœŸ
- âœ… **é€šé…ç¬¦æ”¯æŒ** - `*.parquet` å¯ä»¥è¯»å–å¤šä¸ªåˆ†ç‰‡æ–‡ä»¶

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# ä¸ä¼šç«‹å³ä¸‹è½½æ•°æ®
lazy_df = scan_parquet_from_s3(
    bucket="my-bucket",
    key="nppes/raw/",
    profile="default"
)

# å¯ä»¥å…ˆè¿‡æ»¤ï¼Œåªä¸‹è½½éœ€è¦çš„æ•°æ®
result = lazy_df.filter(pl.col("state") == "CA").collect()
```

**Jay çš„ä¼˜åŠ¿ï¼š**
- åªä¸‹è½½éœ€è¦çš„æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½å’Œæ—¶é—´
- æ”¯æŒæ•°æ®æ¹–æ¶æ„ï¼ˆå¤šæ–‡ä»¶åˆ†ç‰‡ï¼‰

---

## ğŸ¨ Jay çš„ç¼–ç é£æ ¼åˆ†æ

### âœ… ä¼˜ç‚¹

1. **æ³¨é‡Šè¯¦ç»†ä¸”æœ‰ä»·å€¼**
   ```python
   # Uses Polars' streaming engine to reduce memory usage for large datasets
   # (delete=False) doesn't delete it immediately, so it's available for upload
   # rstrip('/') removes trailing slashes for consistent output path
   ```
   â†’ ä¸ä»…è¯´"åšä»€ä¹ˆ"ï¼Œè¿˜è¯´"ä¸ºä»€ä¹ˆ"

2. **ç±»å‹æç¤ºæ¸…æ™°**
   ```python
   def write_parquet_to_s3(
       df: Union[pl.DataFrame, pl.LazyFrame],  # æ˜ç¡®æ”¯æŒä¸¤ç§ç±»å‹
       bucket: str,
       key: str,
       profile: str,
   ) -> str:  # è¿”å› S3 è·¯å¾„
   ```

3. **é˜²å¾¡æ€§ç¼–ç¨‹**
   ```python
   s3_key = f"{key.rstrip('/')}/data.parquet"  # é˜²æ­¢åŒæ–œæ 
   if isinstance(df, pl.LazyFrame):  # ç±»å‹æ£€æŸ¥
   ```

4. **èµ„æºç®¡ç†ä¸¥æ ¼**
   ```python
   with tempfile.NamedTemporaryFile(...) as tmp:  # è‡ªåŠ¨æ¸…ç†
   os.remove(tmp_path)  # åŒé‡ä¿é™©
   ```

### ğŸ’¡ å¯ä»¥å­¦ä¹ çš„åœ°æ–¹

1. **æ¨¡å—åŒ–è®¾è®¡** - æ¯ä¸ªå‡½æ•°èŒè´£å•ä¸€ï¼Œæ˜“äºæµ‹è¯•
2. **æ€§èƒ½æ„è¯†** - è€ƒè™‘å†…å­˜å’Œå¤§æ•°æ®åœºæ™¯
3. **ç”Ÿäº§çº§ä»£ç ** - æ—¥å¿—ã€é”™è¯¯å¤„ç†ã€èµ„æºæ¸…ç†

---

## ğŸ”„ æ•°æ®æµç¨‹å›¾

```
æœ¬åœ°æ•°æ®å¤„ç†æµç¨‹ï¼ˆä½ çš„å·¥ä½œï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data   â”‚  data/raw/npidata_pfile.csv (9.9GB)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ scripts/clean_nppes.py
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cleaned     â”‚  data/Cleaned/*.parquet (123MB)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ scripts/view_parquet_data.py
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis   â”‚  æŸ¥çœ‹å’Œåˆ†æ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

äº‘ç«¯æ•°æ®æµç¨‹ï¼ˆJay çš„å·¥ä½œï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local      â”‚  data/Cleaned/*.parquet
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ src/s3_services.py::write_parquet_to_s3()
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS S3    â”‚  s3://bucket/nppes/clean/
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ src/s3_services.py::scan_parquet_from_s3()
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LazyFrame  â”‚  è¿œç¨‹åˆ†æå’Œå¤„ç†
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤ ä½ ä»¬çš„å·¥ä½œå¦‚ä½•äº’è¡¥

| ä½ çš„å·¥ä½œ | Jay çš„å·¥ä½œ |
|---------|-----------|
| æœ¬åœ°æ•°æ®æ¸…ç† | äº‘ç«¯æ•°æ®å­˜å‚¨ |
| æœ¬åœ°æ•°æ®æŸ¥çœ‹ | äº‘ç«¯æ•°æ®è¯»å– |
| scripts/ æ–‡ä»¶å¤¹ | src/ æ–‡ä»¶å¤¹ |
| æ•°æ®å‡†å¤‡é˜¶æ®µ | æ•°æ®å…±äº«é˜¶æ®µ |

**å®Œæ•´æµç¨‹ï¼š**
1. ä½ ï¼šæ¸…ç†åŸå§‹æ•°æ® â†’ ç”Ÿæˆ Parquet
2. Jayï¼šä¸Šä¼  Parquet åˆ° S3
3. å›¢é˜Ÿï¼šä» S3 è¯»å–æ•°æ®è¿›è¡Œåˆ†æ

---

## ğŸ’¡ å¦‚ä½•ä½¿ç”¨ Jay çš„ä»£ç 

### ç¤ºä¾‹ 1ï¼šä¸Šä¼ æ¸…ç†åçš„æ•°æ®
```python
import polars as pl
from src.s3_services import write_parquet_to_s3

# è¯»å–ä½ æ¸…ç†çš„æ•°æ®
df = pl.read_parquet("data/Cleaned/nppes_cleaned.parquet")

# ä¸Šä¼ åˆ° S3
s3_path = write_parquet_to_s3(
    df=df,
    bucket="de-october-individual-folders",
    key="nppes/clean/",
    profile="default"
)

print(f"Uploaded to: {s3_path}")
```

### ç¤ºä¾‹ 2ï¼šä» S3 è¯»å–å¹¶åˆ†æ
```python
from src.s3_services import scan_parquet_from_s3

# æ‡’åŠ è½½ S3 æ•°æ®
lazy_df = scan_parquet_from_s3(
    bucket="de-october-individual-folders",
    key="nppes/clean/",
    profile="default"
)

# åªä¸‹è½½ California çš„æ•°æ®
ca_providers = (
    lazy_df
    .filter(pl.col("state") == "CA")
    .group_by("taxonomy_code")
    .agg(pl.count())
    .collect()
)

print(ca_providers)
```

---

## ğŸ“š å­¦åˆ°çš„å…³é”®æŠ€æœ¯

1. **boto3** - AWS Python SDK
2. **Polars LazyFrame** - å¤§æ•°æ®å¤„ç†
3. **tempfile** - ä¸´æ—¶æ–‡ä»¶ç®¡ç†
4. **æµå¼å¤„ç†** - å†…å­˜ä¼˜åŒ–
5. **ç±»å‹æç¤º** - Union, typing æ¨¡å—

---

## ğŸ¯ æ€»ç»“

Jay çš„ä»£ç ä½“ç°äº†ï¼š
- âœ… **ç³»ç»Ÿæ€ç»´** - è€ƒè™‘æ•´ä¸ªæ•°æ®ç®¡é“
- âœ… **å·¥ç¨‹èƒ½åŠ›** - å¤„ç†å¤§æ•°æ®å’Œç”Ÿäº§ç¯å¢ƒé—®é¢˜
- âœ… **åä½œæ„è¯†** - åˆ›å»ºå¯å¤ç”¨çš„æ¨¡å—ä¾›å›¢é˜Ÿä½¿ç”¨

è¿™æ˜¯ä¸€ä¸ª**ç”Ÿäº§çº§åˆ«çš„ S3 æ•°æ®æœåŠ¡æ¨¡å—**ï¼Œå€¼å¾—å­¦ä¹ ï¼
